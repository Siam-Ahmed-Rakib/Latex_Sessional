@inproceedings{
louizos2018learning,
title={Learning Sparse Neural Networks through {$L_0$} Regularization},
author={Christos Louizos and Max Welling and Diederik P. Kingma},
booktitle={International Conference on Learning Representations},
year={2018},
}
% removed info from Louizos et al
% url={https://openreview.net/forum?id=H1Y8hhg0b},

@article{JMLR:v22:20-848,
  author  = {Ismael Lemhadri and Feng Ruan and Louis Abraham and Robert Tibshirani},
  title   = {LassoNet: A Neural Network with Feature Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {127},
  pages   = {1-29}
}
% removed info from Lemhadri et al
% url     = {http://jmlr.org/papers/v22/20-848.html}

@inproceedings{1,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Neural Ordinary Differential Equations},
 volume = {31},
 year = {2018}
}
% removed info from Chen et al
% publisher = {Curran Associates, Inc.},
 

@misc{mu2019mnistc,
      title={MNIST-C: A Robustness Benchmark for Computer Vision}, 
      author={Norman Mu and Justin Gilmer},
      year={2019},
      eprint={1906.02337},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{dupont2019augmented,
	title = {Augmented {Neural} {ODEs}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {notion},
}
% removed info:
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2019/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf},

% above is the accepted paper
% @article{dupont2019augmented,
%   title={Augmented Neural ODEs},
%   author={Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
%   journal={arXiv preprint arXiv:1904.01681},
%   year={2019}
% }

@article{grathwohl2019ffjord,
  title={FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
  author={Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{10027939599,
author="LECUN, Y.",
title="THE MNIST DATABASE of handwritten digits",
journal="http://yann.lecun.com/exdb/mnist/",
ISSN="",
publisher="",
year="",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10027939599/en/",
DOI="",
}

@inproceedings{NEURIPS2019_8558cb40,
 author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 title = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
 volume = {32},
 year = {2019}
}
% removed info from Ovadia et al
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf},
 

@inproceedings{NIPS2017_9ef2ed4b,
 author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 volume = {30},
 year = {2017}
}
% removed info from Lakshminarayanan et al
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
 





@inproceedings{kelly2020easynode,
  title={Learning Differential Equations that are Easy to Solve},
  author={Kelly, Jacob and Bettencourt, Jesse and Johnson, Matthew James and Duvenaud, David},
  booktitle={Neural Information Processing Systems},
  year={2020},
}
% removed info from Kelly et al:
%   url={https://arxiv.org/abs/2007.04504}

@inproceedings{norcliffe2021neural,
title={Neural {\{}ODE{\}} Processes},
author={Alexander Norcliffe and Cristian Bodnar and Ben Day and Jacob Moss and Pietro Li{\`o}},
booktitle={International Conference on Learning Representations},
year={2021}
}
% removed info from Norcliffe et al
% url={https://openreview.net/forum?id=27acGyyI1BY}


% @misc{norcliffe2021neural,
%       title={Neural ODE Processes}, 
%       author={Alexander Norcliffe and Cristian Bodnar and Ben Day and Jacob Moss and Pietro Liò},
%       year={2021},
%       eprint={2103.12413},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }


@InProceedings{pal2021opening,
  title = 	 {Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics},
  author =       {Pal, Avik and Ma, Yingbo and Shah, Viral and Rackauckas, Christopher V},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8325--8335},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/pal21a/pal21a.pdf},
  abstract = 	 {Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver’s algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning.}
}
% removed info from Pal et al
% month = 	 {18--24 Jul},
% url = 	 {https://proceedings.mlr.press/v139/pal21a.html},



% @misc{pal2021opening,
%       title={Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics}, 
%       author={Avik Pal and Yingbo Ma and Viral Shah and Christopher Rackauckas},
%       year={2021},
%       eprint={2105.03918},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }


@inproceedings{Norcliffe2020,
	title = {On {Second} {Order} {Behaviour} in {Augmented} {Neural} {ODEs}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Simidjievski, Nikola and Lió, Pietro},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {notion},
	pages = {5911--5921},
}
% removed info from Norcliffe et al
% url = {https://proceedings.neurips.cc/paper/2020/file/418db2ea5d227a9ea8db8e5357ca2084-Paper.pdf},
% publisher = {Curran Associates, Inc.},
		


% @article{Norcliffe2020,
% journal = {arXiv},
% arxivId = {2006.07220},
% author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Simidjievski, Nikola and Li{\`{o}}, Pietro},
% eprint = {2006.07220},
% file = {:Users/hananeh.aliee/Library/Application Support/Mendeley Desktop/Downloaded/Norcliffe et al. - 2020 - On Second Order Behaviour in Augmented Neural ODEs.pdf:pdf},
% month = {jun},
% title = {{On Second Order Behaviour in Augmented Neural ODEs}},
% url = {http://arxiv.org/abs/2006.07220},
% year = {2020}
% }

@article{hoefler2021sparsity,
  author  = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
  title   = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1--124},
}
% removed info: 
% url     = {http://jmlr.org/papers/v22/21-0366.html}

% above is the accepted paper
% @misc{hoefler2021sparsity,
%       title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks}, 
%       author={Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
%       year={2021},
%       eprint={2102.00554},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

@misc{neyshabur2018understanding,
      title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks}, 
      author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
      year={2018},
      eprint={1805.12076},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{Allen-Zhu18,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  abstract = 	 {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100{\%} training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps &nbsp; e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
}
% removed from Allen-Zhu et al:
% pdf = 	 {http://proceedings.mlr.press/v97/allen-zhu19a/allen-zhu19a.pdf},
% url = 	 {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  


% @misc{Allen-Zhu18,
%       title={A Convergence Theory for Deep Learning via Over-Parameterization}, 
%       author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
%       year={2019},
%       eprint={1811.03962},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

@inproceedings{liebenwein2021sparse,
	title = {Sparse {Flows}: {Pruning} {Continuous}-depth {Models}},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liebenwein, Lucas and Hasani, Ramin and Amini, Alexander and Rus, Daniela},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {22628--22642},
}
% removed info from Liebenwein et al
% url = {https://proceedings.neurips.cc/paper/2021/file/bf1b2f4b901c21a1d8645018ea9aeb05-Paper.pdf},
% publisher = {Curran Associates, Inc.},
		


% @misc{liebenwein2021sparse,
%       title={Sparse Flows: Pruning Continuous-depth Models}, 
%       author={Lucas Liebenwein and Ramin Hasani and Alexander Amini and Daniela Rus},
%       year={2021},
%       eprint={2106.12718},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

@article{Scholkopf2021,
  title = {Toward Causal Representation Learning},
  author = {Sch{\"o}lkopf, B. and Locatello, F. and Bauer, S. and Ke, N. R. and Kalchbrenner, N. and Goyal, A. and Bengio, Y.},
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {5},
  pages = {612--634},
  year = {2021},
}
% removed info from Schölkopf et al
% doi = {10.1109/JPROC.2021.3058954},
% url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363924}

% @article{Scholkopf2021,
% journal = {arXiv},
% arxivId = {2102.11107},
% author = {Sch{\"{o}}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
% eprint = {2102.11107},
% month = {feb},
% title = {{Towards Causal Representation Learning}},
%url = {http://arxiv.org/abs/2102.11107},
% year = {2021}
% }

@InProceedings{Li2020,
  title = 	 {Scalable Gradients and Variational Inference for
 Stochastic Differential Equations },
  author =       {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David K.},
  booktitle = 	 {Proceedings of The 2nd Symposium on
 Advances in Approximate Bayesian Inference},
  pages = 	 {1--28},
  year = 	 {2020},
  editor = 	 {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
  volume = 	 {118},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v118/li20a/li20a.pdf},
  abstract = 	 { We derive reverse-mode (or adjoint) automatic differentiation for solutions of stochastic differential equations (SDEs), allowing time-efficient and constant-memory computation of pathwise gradients, a continuous-time analogue of the reparameterization trick. Specifically, we construct a backward SDE whose solution is the gradient and provide conditions under which numerical solutions converge. We also combine our stochastic adjoint approach with a stochastic variational inference scheme for continuous-time SDE models, allowing us to learn distributions over functions using stochastic gradient descent. Our latent SDE model achieves competitive performance compared to existing approaches on time series modeling.}
}
% removed info from Li et al
% url = 	 {https://proceedings.mlr.press/v118/li20a.html},
% month = 	 {08 Dec},
    

% @article{Li2020,
% journal = {arXiv},
% arxivId = {2001.01328},
% author = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
% eprint = {2001.01328},
% month = {jan},
% title = {{Scalable Gradients for Stochastic Differential Equations}},
% url = {http://arxiv.org/abs/2001.01328},
% year = {2020}
% }


@inproceedings{Oganesyan2020,
title={Stochasticity in Neural {ODE}s: An Empirical Study},
author={Alexandra  Volokhova and Viktor Oganesyan and Dmitry Vetrov},
booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
year={2019},
}
% removed info from Oganesyan et al
% url={https://openreview.net/forum?id=C4ydiXrYw}


% @misc{Oganesyan2020,
% author = {Oganesyan, Viktor and Volokhova, Alexandra and Vetrov, Dmitry},
% booktitle = {arXiv},
% issn = {23318422},
% title = {{Stochasticity in Neural ODEs: An Empirical Study}},
% year = {2020}
% }


@inproceedings{Rubanova2019,
	title = {Latent {Ordinary} {Differential} {Equations} for {Irregularly}-{Sampled} {Time} {Series}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David K},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}
% removed info from Rubanova et al
% url = {https://proceedings.neurips.cc/paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf},
% publisher = {Curran Associates, Inc.},
		

% @misc{Rubanova2019,
% author = {Rubanova, Yulia and Chen, Ricky T.Q. and Duvenaud, David},
% booktitle = {arXiv},
% issn = {23318422},
% title = {{Latent ODEs for irregularly-sampled time series}},
% year = {2019}
% }

@misc{aliee2021predictions,
      title={Beyond Predictions in Neural ODEs: Identification and Interventions}, 
      author={Hananeh Aliee and Fabian J. Theis and Niki Kilbertus},
      year={2021},
      eprint={2106.12430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
bellot2021graphical,
title={Neural graphical modelling in continuous-time: consistency guarantees and algorithms},
author={Alexis Bellot and Kim Branson and Mihaela van der Schaar},
booktitle={International Conference on Learning Representations},
year={2022},
}
% removed info
% url={https://openreview.net/forum?id=SsHBkfeRF9L}

% above is the update
% @misc{bellot2021graphical,
%       title={Graphical modelling in continuous-time: consistency guarantees and algorithms using Neural ODEs}, 
%       author={Alexis Bellot and Kim Branson and Mihaela van der Schaar},
%       year={2021},
%       eprint={2105.02522},
%       archivePrefix={arXiv},
%       primaryClass={stat.ML}
% }

@misc{chen2020learning,
      title={Learning Flat Latent Manifolds with VAEs}, 
      author={Nutan Chen and Alexej Klushyn and Francesco Ferroni and Justin Bayer and Patrick van der Smagt},
      year={2020},
      eprint={2002.04881},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{
luecken2021a,
title={A sandbox for prediction and integration of {DNA}, {RNA}, and proteins in single cells},
author={Malte D Luecken and Daniel Bernard Burkhardt and Robrecht Cannoodt and Christopher Lance and Aditi Agrawal and Hananeh Aliee and Ann T Chen and Louise Deconinck and Angela M Detweiler and Alejandro A Granados and Shelly Huynh and Laura Isacco and Yang Joon Kim and Dominik Klein and BONY DE KUMAR and Sunil Kuppasani and Heiko Lickert and Aaron McGeever and Honey Mekonen and Joaquin Caceres Melgarejo and Maurizio Morri and Michaela M{\"u}ller and Norma Neff and Sheryl Paul and Bastian Rieck and Kaylie Schneider and Scott Steelman and Michael Sterr and Daniel J. Treacy and Alexander Tong and Alexandra-Chloe Villani and Guilin Wang and Jia Yan and Ce Zhang and Angela Oliveira Pisco and Smita Krishnaswamy and Fabian J Theis and Jonathan M. Bloom},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
}
% removed info from Luecken et al
% url={https://openreview.net/forum?id=gN35BGa1Rt}



@ARTICLE{Lambert2018-nc,
  title    = "The Human Transcription Factors",
  author   = "Lambert, Samuel A and Jolma, Arttu and Campitelli, Laura F and
              Das, Pratyush K and Yin, Yimeng and Albu, Mihai and Chen,
              Xiaoting and Taipale, Jussi and Hughes, Timothy R and Weirauch,
              Matthew T",
  abstract = "Transcription factors (TFs) recognize specific DNA sequences to
              control chromatin and transcription, forming a complex system
              that guides expression of the genome. Despite keen interest in
              understanding how TFs control gene expression, it remains
              challenging to determine how the precise genomic binding sites of
              TFs are specified and how TF binding ultimately relates to
              regulation of transcription. This review considers how TFs are
              identified and functionally characterized, principally through
              the lens of a catalog of over 1,600 likely human TFs and binding
              motifs for two-thirds of them. Major classes of human TFs differ
              markedly in their evolutionary trajectories and expression
              patterns, underscoring distinct functions. TFs likewise underlie
              many different aspects of human physiology, disease, and
              variation, highlighting the importance of continued effort to
              understand TF-mediated gene regulation.",
  journal  = "Cell",
  volume   =  172,
  number   =  4,
  pages    = "650--665",
  month    =  feb,
  year     =  2018
}


@article{Aalto2020,
author = {Aalto, Atte and Viitasaari, Lauri and Ilmonen, Pauliina and Mombaerts, Laurent and Gon{\c{c}}alves, Jorge},
journal = {Nature Communications},
issn = {2041-1723},
number = {1},
pages = {3493},
title = {{Gene regulatory network inference from sparsely sampled noisy data}},
volume = {11},
year = {2020}
}
% removed info for Aalto et al:
% doi = {10.1038/s41467-020-17217-1},
% url = {https://doi.org/10.1038/s41467-020-17217-1},


@article{QIU2020265,
title = {Inferring Causal Gene Regulatory Networks from Coupled Single-Cell Expression Dynamics Using Scribe},
journal = {Cell Systems},
volume = {10},
number = {3},
pages = {265-274.e11},
year = {2020},
issn = {2405-4712},
author = {Xiaojie Qiu and Arman Rahimzamani and Li Wang and Bingcheng Ren and Qi Mao and Timothy Durham and JosÃ© L. McFaline-Figueroa and Lauren Saunders and Cole Trapnell and Sreeram Kannan}
}
% removed info from Qiu et al
% doi = {https://doi.org/10.1016/j.cels.2020.02.003},
% url = {https://www.sciencedirect.com/science/article/pii/S2405471220300363},



@misc{shell2012carnegie,
  title={Carnegie Mellon University motion capture database},
  author={Shell, M},
  year={2012},
  publisher={mocap. cs. cmu. edu}
}

@article{Wolf2018,
author = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
issn = {1474760X},
journal = {Genome Biology},
pmid = {29409532},
title = {{SCANPY: Large-scale single-cell gene expression data analysis}},
year = {2018}
}
% removed info from Wolf et al
% doi = {10.1186/s13059-017-1382-0},


@inproceedings{kingma2017adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year      = {2015},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% removed info
% url       = {http://arxiv.org/abs/1412.6980},
% San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings

% above is the accepted paper
% @misc{kingma2017adam,
%       title={Adam: A Method for Stochastic Optimization}, 
%       author={Diederik P. Kingma and Jimmy Ba},
%       year={2017},
%       eprint={1412.6980},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

@article{Haghverdi2016,
author = {Haghverdi, Laleh and B{\"{u}}ttner, Maren and Wolf, F. Alexander and Buettner, Florian and Theis, Fabian J.},
issn = {15487105},
journal = {Nature Methods},
number = {10},
title = {{Diffusion pseudotime robustly reconstructs lineage branching}},
volume = {13},
year = {2016}
}
% removed info
% doi = {10.1038/nmeth.3971},


@inproceedings{bartoldson2020generalizationstability,
	title = {The {Generalization}-{Stability} {Tradeoff} {In} {Neural} {Network} {Pruning}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {20852--20864},
}
% now NIPS, and removed info for Bartoldson et al:
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2020/file/ef2ee09ea9551de88bc11fd7eeea93b0-Paper.pdf},
	
@misc{Subutai16,
  author = {Ahmad, Subutai and Hawkins, Jeff},
  title = {How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites},
  publisher = {arXiv},
  year = {2016}
}
% removed info for Ahmad et al:
% doi = {10.48550/ARXIV.1601.00720},
% url = {https://arxiv.org/abs/1601.00720},
  
@article{Zhang16,
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {3},
issn = {0001-0782},
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.We interpret our experimental findings by comparison with traditional models.We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
journal = {Commun. ACM},
pages = {107–115},
numpages = {9}
}
% removed info from Zhang et al
% url = {https://doi.org/10.1145/3446776},
% doi = {10.1145/3446776},
% month = {feb},


% @misc{Zhang16,
%   doi = {10.48550/ARXIV.1611.03530},
%   author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
%   title = {Understanding deep learning requires rethinking generalization},
%   publisher = {arXiv},
%   year = {2016}
% }

@inproceedings{Li18,
	title = {Learning and {Generalization} in {Overparameterized} {Neural} {Networks}, {Going} {Beyond} {Two} {Layers}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}
% now NIPS & removed info:
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2019/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf},
	


% @misc{Li18,
%   doi = {10.48550/ARXIV.1808.01204},
%   author = {Li, Yuanzhi and Liang, Yingyu},
%   title = {Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
%   publisher = {arXiv},
%   year = {2018}
% }


@inproceedings{Denil13,
	title = {Predicting {Parameters} in {Deep} {Learning}},
	volume = {26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc' Aurelio and de Freitas, Nando},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}
% removed info:
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf},

% above is the accepted paper
% @misc{Denil13,
%   author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and de Freitas, Nando},
%   title = {Predicting Parameters in Deep Learning},
%   publisher = {arXiv},
%   year = {2013}
% }

@inproceedings{Finlay20,
  author={Chris Finlay and Jörn-Henrik Jacobsen and Levon Nurbekyan and Adam M. Oberman},
  title={How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization},
  year={2020},
  booktitle={ICML},
  cdate={1577836800000},
  pages={3154-3164},
  crossref={conf/icml/2020}
}
% removed info:
% url={http://proceedings.mlr.press/v119/finlay20a.html},
  

% above is the accepted paper
% @misc{Finlay20,
%   author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
%   title = {How to train your neural ODE: the world of Jacobian and kinetic regularization},
%   publisher = {arXiv},
%   year = {2020}
% }


@inproceedings{Arnab20,
	title = {{STEER} : {Simple} {Temporal} {Regularization} {For} {Neural} {ODE}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ghosh, Arnab and Behl, Harkirat and Dupont, Emilien and Torr, Philip and Namboodiri, Vinay},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {14831--14843},
}
% removed info
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2020/file/a9e18cb5dd9d3ab420946fa19ebbbf52-Paper.pdf},

% above is the accepted paper
% @misc{Arnab20,
%   author = {Ghosh, Arnab and Behl, Harkirat Singh and Dupont, Emilien and Torr, Philip H. S. and Namboodiri, Vinay},
%   title = {STEER: Simple Temporal Regularization For Neural ODEs},
%   publisher = {arXiv},
%   year = {2020}
% }

@article{Lim_2021,
	year = 2021,
	publisher = {The Royal Society},
	volume = {379},
	number = {2194},
	pages = {20200209},
	author = {Bryan Lim and Stefan Zohren},
	title = {Time-series forecasting with deep learning: a survey},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}
% removed info from Lim et al
% month = {feb},
  

@inproceedings{Ng20,
	title = {On the {Role} of {Sparsity} and {DAG} {Constraints} for {Learning} {Linear} {DAGs}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ng, Ignavier and Ghassami, AmirEmad and Zhang, Kun},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {17943--17954},
}
% removed info from Ng et al
% publisher = {Curran Associates, Inc.},
% url = {https://proceedings.neurips.cc/paper/2020/file/d04d42cdf14579cd294e5079e0745411-Paper.pdf},

% @misc{Ng20,
%   author = {Ng, Ignavier and Ghassami, AmirEmad and Zhang, Kun},
%   title = {On the Role of Sparsity and DAG Constraints for Learning Linear DAGs},
%   publisher = {arXiv},
%   year = {2020}
% }

@article{Bolstad_2011,
	year = 2011,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {59},
	number = {6},
	pages = {2628--2641},
	author = {A Bolstad and B D Van Veen and R Nowak},
	title = {Causal Network Inference Via Group Sparse Regularization},
	journal = {{IEEE} Transactions on Signal Processing}
}
% removed info from Bolstad et al
% month = {jun},
  

@article{Hasani_Lechner_Amini_Rus_Grosu_2021, 
title={Liquid Time-constant Networks}, volume={35},
url={https://ojs.aaai.org/index.php/AAAI/article/view/16936}, 
abstractNote={We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system’s dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., liquid) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics, and compute their expressive power by the trajectory length measure in a latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to classical and modern RNNs.},
number={9}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
year={2021}, 
month={May}, 
pages={7657-7666} }

@inproceedings{
vorbach2021causal,
title={Causal Navigation by Continuous-time Neural Networks},
author={Charles J Vorbach and Ramin Hasani and Alexander Amini and Mathias Lechner and Daniela Rus},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ckVbQs5zD7_}
}

@InProceedings{Zheng19,
  title = 	 {Learning Sparse Nonparametric DAGs},
  author =       {Zheng, Xun and Dan, Chen and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3414--3425},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  abstract = 	 {We develop a framework for learning sparse nonparametric directed acyclic graphs (DAGs) from data. Our approach is based on a recent algebraic characterization of DAGs that led to the first fully continuous optimization for score-based learning of DAG models parametrized by a linear structural equation model (SEM). We extend this algebraic characterization to nonparametric SEM by leveraging nonparametric sparsity based on partial derivatives, resulting in a continuous optimization problem that can be applied to a variety of nonparametric and semiparametric models including GLMs, additive noise models, and index models as special cases. Unlike existing approaches that require specific modeling choices, loss functions, or algorithms, we present a completely general framework that can be applied to general nonlinear models (e.g. without additive noise), general differentiable loss functions, and generic black-box optimization routines.}
}
% removed info from Zheng et al
% month = 	 {26--28 Aug},
% pdf = 	 {http://proceedings.mlr.press/v108/zheng20a/zheng20a.pdf},
% url = 	 {https://proceedings.mlr.press/v108/zheng20a.html},
    

% @misc{Zheng19,
%   author = {Zheng, Xun and Dan, Chen and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
%   title = {Learning Sparse Nonparametric DAGs},
%   publisher = {arXiv},
%   year = {2019}
% }

@misc{Zheng18,
  title = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
  author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
  publisher = {arXiv},
  year = {2018}
}

@inproceedings{mooij2013ordinary,
author = {Mooij, Joris M. and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
title = {From Ordinary Differential Equations to Structural Causal Models: The Deterministic Case},
year = {2013},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show how, and under which conditions, the equilibrium states of a first-order Ordinary Differential Equation (ODE) system can be described with a deterministic Structural Causal Model (SCM). Our exposition sheds more light on the concept of causality as expressed within the framework of Structural Causal Models, especially for cyclic models.},
booktitle = {Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
pages = {440–448},
numpages = {9},
location = {Bellevue, WA},
series = {UAI'13}
}

% @article{mooij2013ordinary,
%   title={From ordinary differential equations to structural causal models: the deterministic case},
%   author={Mooij, Joris M and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
%   journal={arXiv preprint arXiv:1304.7920},
%   year={2013}
% }

@Inbook{Montavon2019,
author="Montavon, Gr{\'e}goire
and Binder, Alexander
and Lapuschkin, Sebastian
and Samek, Wojciech
and M{\"u}ller, Klaus-Robert",
editor="Samek, Wojciech
and Montavon, Gr{\'e}goire
and Vedaldi, Andrea
and Hansen, Lars Kai
and M{\"u}ller, Klaus-Robert",
title="Layer-Wise Relevance Propagation: An Overview",
bookTitle="Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="193--209"
}

@inproceedings{xu2022sparse,
title={Sparse Neural Additive Model: Interpretable Deep Learning with Feature Selection via Group Sparsity},
author={Shiyun Xu and Zhiqi Bu and Pratik Chaudhari and Ian J. Barnett},
booktitle={ICLR 2022 Workshop on PAIR{\textasciicircum}2Struct: Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data},
year={2022},
}
% removed info from Xu et al
% url={https://openreview.net/forum?id=H6MlMaB6Seq}

% @article{xu2022sparse,
%   title={Sparse Neural Additive Model: Interpretable Deep Learning with Feature Selection via Group Sparsity},
%   author={Xu, Shiyun and Bu, Zhiqi and Chaudhari, Pratik and Barnett, Ian J},
%   journal={arXiv preprint arXiv:2202.12482},
%   year={2022}
% }

@article{brunton2016discovering,
  title={Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Proceedings of the national academy of sciences},
  volume={113},
  number={15},
  pages={3932--3937},
  year={2016},
  publisher={National Acad Sciences}
}

@inproceedings{
Brutzkus,
title={{SGD} Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
booktitle={International Conference on Learning Representations},
year={2018},

}
% removed info from Brutzkus et al:
% url={https://openreview.net/forum?id=rJ33wwxRb},

% @misc{Brutzkus,
%   author = {Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
%   title = {SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
  
%   publisher = {arXiv},
  
%   year = {2017}
% }

@inproceedings{
Du18,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
}
% removed info from Du et al
% url={https://openreview.net/forum?id=S1eK3i09YQ},

% above is the accepted paper
% @article{Du18,
%   author    = {Simon S. Du and
%                Xiyu Zhai and
%                Barnab{\'{a}}s P{\'{o}}czos and
%                Aarti Singh},
%   title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
%   journal   = {CoRR},
%   volume    = {abs/1810.02054},
%   year      = {2018},
%   eprinttype = {arXiv}
% }

@article{Zheng2006,
   abstract = {The zinc finger transcription factor GATA-1 is essential for both primitive (embryonic) and definitive (adult) erythropoiesis. To define the roles of GATA-1 in the production and differentiation of primitive and definitive erythrocytes, we established GATA-1-null embryonic stem cell lines in which GATA-1 was able to be conditionally expressed by using the tetracycline conditional gene expression system. The cells were subjected to hematopoietic differentiation by coculturing on OP9 stroma cells. We expressed GATA-1 in the course of primitive and definitive erythropoiesis and analyzed the ability of GATA-1 to rescue the defective erythropoiesis caused by the GATA-1 null mutation. Our results show that GATA-1 functions in the proliferation and maturation of erythrocytes in a distinctive manner. The early-stage expression of GATA-1 during both primitive and definitive erythropoiesis was sufficient to promote the proliferation of red blood cells. In contrast, the late-stage expression of GATA-1 was indispensable to the terminal differentiation of primitive and definitive erythrocytes. Thus, GATA-1 affects the proliferation and differentiation of erythrocytes by different mechanisms. Â© 2006 by The American Society of Hematology.},
   author = {Jie Zheng and Kenji Kitajima and Eiko Sakai and Tohru Kimura and Naoko Minegishi and Masayuki Yamamoto and Toru Nakano},
   issn = {00064971},
   issue = {2},
   journal = {Blood},
   title = {Differential effects of GATA-1 on proliferation and differentiation of erythroid lineage cells},
   volume = {107},
   year = {2006},
}
% removed info from Zheng et al
% doi = {10.1182/blood-2005-04-1385},
   
@article{Micci2013,
   author = {F. Micci and J. Thorsen and I. Panagopoulos and K. B. Nyquist and B. Zeller and A. Tierens and S. Heim},
   issn = {08876924},
   issue = {4},
   journal = {Leukemia},
   title = {High-throughput sequencing identifies an NFIA/CBFA2T3 fusion gene in acute erythroid leukemia with t(1;16)(p31;q24)},
   volume = {27},
   year = {2013},
}
% removed info from Micci et al
% doi = {10.1038/leu.2012.266},
   
@article{Deen2021,
   abstract = {Erythropoiesis requires a combination of ubiquitous and tissue-specific transcription factors (TFs). Here, through DNA affinity purification followed by mass spectrometry, we have identified the widely expressed protein MAZ (Myc-associated zinc finger) as a TF that binds to the promoter of the erythroid-specific human a-globin gene. Genome-wide mapping in primary human erythroid cells revealed that MAZ also occupies active promoters as well as GATA1-bound enhancer elements of key erythroid genes. Consistent with an important role during erythropoiesis, knockdown of MAZ reduces a-globin expression in K562 cells and impairs differentiation in primary human erythroid cells. Genetic variants in the MAZ locus are associated with changes in clinically important human erythroid traits. Taken together, these findings reveal the zinc-finger TF MAZ to be a previously unrecognized regulator of the erythroid differentiation program.},
   author = {Darya Deen and Falk Butter and Deborah E. Daniels and Ivan Ferrer-Vicens and Daniel C.J. Ferguson and Michelle L. Holland and Vasiliki Samara and Jacqueline A. Sloane-Stanley and Helena Ayyub and Matthias Mann and Jan Frayne and David Garrick and Douglas Vernimmen},
   issn = {24739537},
   issue = {15},
   journal = {Blood Advances},
   title = {Identification of the transcription factor MAZ as a regulator of erythropoiesis},
   volume = {5},
   year = {2021},
}
% removed info from Deen et al:
% doi = {10.1182/bloodadvances.2021004609},

@article{Wang1998,
   abstract = {The TEL (translocation-Ets-leukemia or ETV6) locus, which encodes an Ets family transcription factor, is frequently rearranged in human leukemias of myeloid or lymphoid origins. By gene targeting in mice, we previously showed that TEL(-/-) mice are embryonic lethal because of a yolk sac angiogenic defect. TEL also appears essential for the survival of selected neural and mesenchymal populations within the embryo proper. Here, we have generated mouse chimeras with TEL(-/-) ES cells to examine a possible requirement in adult hematopoiesis. Although not required for the intrinsic proliferation and/or differentiation of adult-type hematopoietic lineages in the yolk sac and fetal liver, TEL function is essential for the establishment of hematopoiesis of all lineages in the bone marrow. This defect is manifest within the first week of postnatal life. Our data pinpoint a critical role for TEL in the normal transition of hematopoietic activity from fetal liver to bone marrow. This might reflect an inability of TEL(-/-) hematopoietic stem cells or progenitors to migrate or home to the bone marrow or, more likely, the failure of these cells to respond appropriately and/or survive within the bone marrow microenvironment. These data establish TEL as the first transcription factor required specifically for hematopoiesis within the bone marrow, as opposed to other sites of hematopoietic activity during development.},
   author = {Li Chun Wang and Wojciech Swat and Yuko Fujiwara and Laurie Davidson and Jane Visvader and Frank Kuo and Fred W. Alt and D. Gary Gilliland and Todd R. Golub and Stuart H. Orkin},
   issn = {08909369},
   issue = {15},
   journal = {Genes and Development},
   title = {The TEL/ETV6 gene is required specifically for hematopoiesis in the bone marrow},
   volume = {12},
   year = {1998},
}
% removed info from Wang et al
% doi = {10.1101/gad.12.15.2392},
   
@article{Kuvardina2015,
   
   author = {Olga N. Kuvardina and Julia Herglotz and Stephan Kolodziej and Nicole Kohrs and Stefanie Herkt and Bartosch Wojcik and Thomas Oellerich and Jasmin Corso and Kira Behrens and Ashok Kumar and Helge Hussong and Henning Urlaub and Joachim Koch and Hubert Serve and Halvard Bonig and Carol Stocking and Michael A. Rieger and Jarn Lausen},
   issn = {15280020},
   issue = {23},
   journal = {Blood},
   title = {RUNX1 represses the erythroid gene expression program during megakaryocytic differentiation},
   volume = {125},
   year = {2015},
}

@article{Liu2022,
   author = {Yijia Liu and Lexin Li and Xiao Wang},
   doi = {10.1002/cjs.11666},
   issn = {1708945X},
   issue = {1},
   journal = {Canadian Journal of Statistics},
   title = {A nonlinear sparse neural ordinary differential equation model for multiple functional processes},
   volume = {50},
   year = {2022},
}


@inproceedings{Ross17,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-17},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	year = {2017},
	pages = {2662--2670},
}
% removed info from Ross et al
% url = {https://doi.org/10.24963/ijcai.2017/371},
% doi = {10.24963/ijcai.2017/371},
	

% @misc{Ross17,
%   author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
%   title = {Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations},
%   publisher = {arXiv},
%   year = {2017}
% }


@article{Ross17_2,
	title = {Improving the {Adversarial} {Robustness} and {Interpretability} of {Deep} {Neural} {Networks} by {Regularizing} {Their} {Input} {Gradients}},
	volume = {32},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ross, Andrew and Doshi-Velez, Finale},
	year = {2018},
}
% removed info from Ross et al
% url = {https://ojs.aaai.org/index.php/AAAI/article/view/11504},
% doi = {10.1609/aaai.v32i1.11504},
% month = apr,
	

% @misc{Ross17_2,
%   author = {Ross, Andrew Slavin and Doshi-Velez, Finale},
%   title = {Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients},
%   publisher = {arXiv},
%   year = {2017}
% }


@book{Slavin2018TheNL,
	title = {The {Neural} {LASSO}: {Local} {Linear} {Sparsity} for {Interpretable} {Explanations}},
	author = {Ross, Andrew Slavin and Lage, Isaac and Doshi-Velez, Finale},
	year = {2017},
	note = {Publication Title: Neural Information Processing Systems (NIPS) Workshop on Transparent and Interpretable Machine Learning in Safety Critical Environments},
}


% @article{Slavin2018TheNL,
%   journal={arXiv},
%   title={The Neural LASSO : Local Linear Sparsity for Interpretable Explanations},
%   author={Andrew B Slavin},
%   year={2018}
% }

@misc{Oganesyan20,
  doi = {10.48550/ARXIV.2002.09779},
  author = {Oganesyan, Viktor and Volokhova, Alexandra and Vetrov, Dmitry},
  title = {Stochasticity in Neural ODEs: An Empirical Study},
  publisher = {arXiv},
  year = {2020}
}

@article{xu2021characteristic,
  title={Characteristic Neural Ordinary Differential Equations},
  author={Xu, Xingzi and Hasan, Ali and Elkhalil, Khalil and Ding, Jie and Tarokh, Vahid},
  journal={arXiv preprint arXiv:2111.13207},
  year={2021}
}


@inproceedings{bilos_neural_2021,
	title = {Neural {Flows}: {Efficient} {Alternative} to {Neural} {ODEs}},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Biloš, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and Günnemann, Stephan},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {notion},
	pages = {21325--21337},
}
% removed from Bilos et al
% url = {https://proceedings.neurips.cc/paper/2021/file/b21f9f98829dea9a48fd8aaddc1f159d-Paper.pdf},
	
@article{queiruga2020continuous,
  title={Continuous-in-depth neural networks},
  author={Queiruga, Alejandro F and Erichson, N Benjamin and Taylor, Dane and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2008.02389},
  year={2020}
}

@inproceedings{maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@inproceedings{jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

@article{Vierstra2020,
author = {Vierstra, Jeff and Lazar, John and Sandstrom, Richard and Halow, Jessica and Lee, Kristen and Bates, Daniel and Diegel, Morgan and Dunn, Douglas and Neri, Fidencio and Haugen, Eric and Rynes, Eric and Reynolds, Alex and Nelson, Jemma and Johnson, Audra and Frerker, Mark and Buckley, Michael and Kaul, Rajinder and Meuleman, Wouter and Stamatoyannopoulos, John A},
doi = {10.1038/s41586-020-2528-x},
issn = {1476-4687},
journal = {Nature},
number = {7818},
pages = {729--736},
title = {{Global reference mapping of human transcription factor footprints}},
url = {https://doi.org/10.1038/s41586-020-2528-x},
volume = {583},
year = {2020}
}